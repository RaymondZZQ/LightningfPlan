## 						 DewarpNet: 使用堆叠3D和2D回归网络的单张文本图像矫正

**摘要** ：在非结构化的环境中使用手持设备捕获文档图像在当今非常普遍，然而，太过“随意”的文档图片常常不适合信息的自动提取，主要是因为这些图片会存在物理变形、摄像机位置角度和光照条件变化。**本文中，我们提出了DewarpNet，一个用于单张文档图像矫正的深度学习的方法**。**我们认为文档的3D几何形状不仅决定了图像的纹理变形，而且还导致了光照影响**。 **因此，我们的新颖之处在于直接使用了一个端到端的框架对文档图像的3D形状进行建模。** **此外，我们贡献了一个最大最全面文档图像矫正数据集-Doc3D**

* **1、介绍**

  ​		纸质文档包含了重要的信息并在我们日常工作和生活中扮演了重要的角色。数字化的文档图像可以以一种方便的、安全的和高效的方式来归档、恢复和分享。随着便携式相机和智能手机越来越流行，通过拍照的文档数字化变得更加接近用户。拍摄的文档图像可以被转化为电子格式，如PDF，用于进一步的处理、交换、信息提取和内容分析。当拍摄图像时，尽可能准确的保存文档上的信息(即使用平面扫描版本差异极小)是非常值得称赞的。 然而，由于不可控制的因素比如纸张的物理变形、摄像机的位置角度变化和无约束的光照条件等，使用手机等设备随便拍摄的图像总是会受到不同程度的变形。因此，这些原始图像通常不适合用于信息自动提取和内容分析。

  ​		先前的文献已经采用了各种各样的方法来研究文档矫正问题。传统的方法常常依赖于文档图像的几何性质来恢复扭曲的图像。这些方法首先预测文档的3D形状，有参数化的形状表示和非参数化的形状表示。然后，他们通过变形的图像计算出平整图像，并使用优化技术来预测形状。这些方法普遍的缺点是由于优化过程导致计算量大，耗费时间长。Ma等人最近提出了一个基于深度学习的系统直接通过变形的文档图像回归预测仿射操作。这个方法显著地提升了文档仿射矫正系统的速度，但是并没有遵循图像仿射的3D几何性质--训练集是通过2D变形图像制作的，在测试中常常会出现生成不真实的结果。

  ​		纸张折叠发生在3维的情况：具有不同纹理但3D形状相同的图像可以使用同一个形变场来矫正。因此，3D形状可能是恢复扭曲图像最重要的线索。基于这个思路(idea)，我们提出了DewarpNet, 一个新的数据驱动的矫正框架，这个框架直接使用3D形状表示来学习矫正操作。DewarpNet分两个阶段运行，分别使用两个子网络：

  ​		**（1）“shape network"**，输入变形的文本图像，输出一个3D坐标图，这个图被证明足够完成矫正任务

  ​		**（2）“texture mapping network"**, 将变形的文本图像反向映射到一个平整的文本图像。

  ​		我们使用中间3D形状和最后的矫正结果的回归损失函数来联合训练这两个网络。我们还提供了一个"优化网络"(refinement network）用于移除矫正后的图像的阴影效果，进一步提高结果的感性质量。

  ​		为了能够使用中间的3D形状表示来训练这个矫正网络，我们制作了目前为止最大、最完整的用于文本图像矫正的数据集Doc3D。 我们采用了混合的方式来收集数据：结合

  ​		（1）从自然扭曲的纸张中捕获3D形状(网格)

  ​		（2）对广泛收集的文档内容进行逼真的透视变换

  ​		每一个数据点都包含丰富的标注，包括：3D坐标图，平面图，UV纹理图和albedo图。总计包含10万张图。

  ​		**本文贡献总结如下：**

  ​		**1.** 我们制作了数据集Doc3D

  ​		**2.** 我们提出了DewarpNet,高质量，实时

  ​		**3.** 通过训练集训练的DewarpNet，和最近state-of-the-art方法相比，显示了更加优越的性能。和真实扫描的文档相比，我们提高了15%的M-SSIM和降低了36%的局部变形。 此外，我们证明了我们的方法的实际意义，OCR字符错误率降低了42%。 

* **2、前人的工作**

  ​		根据如何对变形进行建模，之前的文档矫正算法可以分为两组：基于参数化的形状模型和基于非参数化的形状模型。

  ​		**（1）基于参数化形状的方法：** 假设文档的变形可以通过低维的参数化模型来表示，且模型的参数可以使用视觉上的线索来推导。如圆柱面是最普遍的参数化模型，其他的还有NURBS模型、分段自然三次曲线模型NCS、Coon patches等。用于预测模型参数的***视觉线索***包括文本线、文档边界或者外部设备发出的激光束等。Shafait[33]展示了几个参数形状方法，这些方法在只有透视和曲面变形的小数据集上验证。然而，这样的低维模型很难对复杂表面变形进行建模。

  ​		**（2）基于非参数化形状的方法：**与基于参数化形状的方法相反，不依赖于低维的参数模型。这类方法使用网格来表示变形的文档图像，并且直接预测每个网格顶点的位置。预测网格顶点的方法包含有参考图像、文本行和CNNs。许多方法通过预测的或者捕获的3D纸张形状信息重建网格。值得注意的例子如通过立体视觉的点云预测、多视角图像、结构光和激光范围扫描等。还有方法直接使用纹理信息来完成矫正任务。然而借助额外设备或者多视角图像使得方法不太使用，局部文本行特征也不能较好处理图文混合的文档图像。此外，这些方法通常还包含复杂且耗时的优化。最近，Ma[23]提出了DocUNet，这个方法是第一个数据驱动的用于处理文档矫正的深度学习方法。与之前的方法相比，DocUNet在推理阶段更快但在真实场景的图像上并总是取得好的效果，主要是因为合成的训练集只包含了2D变形。

* **3、数据集介绍**

  > ​		我们使用一个混合的方式来制作数据集Doc3D，同时使用真实文档图像和渲染软件。 我们首先获取真实自然变形的文档图像的3D形状(网格)，然后我们使用Blender[1]中的路径追踪来渲染具有正式文档纹理的图像，在渲染过程中，我们使用不同摄像机位置和不同的光照条件。
  >
  > ​		我们的方法有一个显著的好处是制作了一个大规模的逼真的数据集，与此同时，生成了多种类型的像素级文本真实标签，包括***3D坐标图、反射率图、法线、深度图和UV图***。这些图像的形成差异对我们的任务很有帮助，但是在真实场景下很难获得。
  >
  > ​		和文献[23]中的数据集相比，他们的3D变形仅通过2D建模，我们的数据集采用了模拟文档物理变形的方式。因此，与[23]中的数据集相比，我们有理由认为在我们数据集上训练的深度学习模型能够在真实环境下测试时获得更好的性能。图2是直观的比较数据样本。
  >
  > ![alt 图像Fig2](D:\新建文件夹\OCR\论文\data\插图1.jpg)
  >
  > **3.1 捕获变形文档图像的3D形状**
  >
  > 	> ​		**（1）3D点云捕获**   我们用于捕获变形的文档形状的工作站由 一个工作台、一个支架、一个深度摄像头和支撑架组成。支架用于控制深度摄像头的高度、以58里面的高度正对着工作台。在这个高度上，深度摄像头可捕获整个文档区域并能保存变形细节。 支撑架有64个可独立控制的针脚，这些针脚可以提升文档的高度并和工作台分开。 高度差异使得从深度图的背景中提取文档更加容易。 这个架子可以模拟复杂的静止表面，并可以支持变形的文档保持视觉线索和褶皱细节。
  > 	>
  > 	> ​		我们使用校准过的Inter RealSenseD415深度摄像头来捕获深度图。假设没有遮挡，则文档的点云可以表示为：X-3D=K[i,j,dij],其中dij是位置i,j处的深度。矩阵K是从摄像头中读取的。我们对6帧图像求均值来减小零均值噪声，并使用高斯核的移动最小二乘来使得点云更加平滑(模糊)
  > 	>
  > 	> ​		**(2) 网格制作**  我们使用滚球算法(ball pivoting algorithm[3])从捕获到的点云中提取网格。网格有大约13万个顶点和27万个面几乎覆盖所有的顶点。 然后，我们对每个网格进行二次采样得到100X100的统一网格来促进网格的增强、对齐和渲染。由于我们的廉价的传感器的精度限制，即使更高的网格分辨率也不能提供更好的细节，如细小折痕等。 每个顶点都有一个UV位置表示纹理坐标，用于渲染阶段的纹理映射。 假设(u,v)={(0,0),(0,1),(1,0),(1,1)}是网格的四个角顶点，我们对所有的网格进行UV插值。
  > 	>
  > 	> ​		**（3）网格增强和对齐**  为了进一步利用每个网格，我们首先分别沿着x,y,z三个轴进行翻转得到8个网格，然后随机裁剪4个不同横纵比且大小在65X65到95X95之间的小网格，将所有网格进行插值到同一个分辨率大小100X100. 这些额外的网格明显地增加了数据集的多样性。所有的网格都通过解决一个绝对方向问[13]的方式一个 对齐到模板网格，统一尺度、角度和平移。 这一步确保了每一个独一无二的变形都有一个独一无二的3D坐标表示。 我们一共生成了40000个不同的网格。
  >
  > 
  >
  > **3.2  文档图像渲染**
  >
  > > ​		**(1) 配置**   为了增加数据集的多样性，我们改变了渲染过程中的相机、光照和纹理配置。 对于每一张图像，相机向上偏转[-30,30]度并随机放置在球冠上。相机的方向被限制在一个虚拟世界周围的小区域中。 我们从数据集Laval Indoor HDR[12]的2100种环境图中随机采样照明环境对70%的图像进行渲染。 我们也在简单的照明条件如随机采样的点光源下对30%的图像进行渲染。 网格上的纹理都是从真实世界中的文档图像中获取。 我们共收集了7200张图像，包含学术文章、杂志、邮件和书籍等，包含了图文混合的多个布局。
  > >
  > > ​		**（2）丰富的标注**   对于每张图像，我们生成了   3D坐标图、深度图、法线、UV映射和albedo反照率图。下一节，我们将展示如何在我们的网络中使用这些标签。

  

  > <img src="D:\新建文件夹\OCR\论文\data\插图2.jpg" alt="alt " style="zoom:150%;" />

  

* **4、DewarpNet**

  ![alt tu](D:\新建文件夹\OCR\论文\data\插图3.jpg)

  > **4.1 网络架构**
  >
  > ​		如图所示，DewarpNet包含两个子网络：shape network和Texture mapping network， 此外，我们还提出了一个后置的优化模块用于调整光照影响，增强矫正后图像的视觉效果
  >
  > ​		**DewarpNet**将变形的图像 I（H\*W\*3）作为输入，且预测一个方向映射B(H\*W\*2). B是一个表示图像变形的流场：B上每个位置的元素(x,y)都表示一个在输入图像*I*上的像素位置。 我们使用双线性采样来对图像I上的像素值进行采样，生成最终的矫正后的图像
  >
  > ​		**shape network** 形状回归网络。 DewarpNet首先回归输入文档图像的3D形状，我们将这个回归任务看作是一个***image-to-image***转换问题：给定一个输入图像*I*，shape network将其的每个元素都转换到3D坐标图C(H\*W\*3)中，C中的每个像素值(X,Y,Z)都对应3D坐标文档形状，如图4所示。我们使用一个U-Net类型的编码-解码架构，在shape network的编码和解码之间有跳跃连接
  >
  > ​		**texture mapping network**  纹理映射网络。纹理映射网络的输入是3D坐标图C，输出的是反向映射B。在纹理映射网络中，我们使用带有多个DenseNet模块的编码-解码架构。这个任务是从3D坐标图C到纹理坐标B的坐标转换。我们在纹理映射网络中使用***坐标卷积(Coordinate Convolution，CoordConv)*** ，CoordConv已经被证明在坐标转换任务[18,22]中可以提高网络的泛化能力，我们的实验显示这个技术的效果。
  >
  > ​		**Refinement Network 优化网络。** 优化网络在我们的系统中作为一个后处理的组件用于调整矫正过的图像的光照影响。 这个网络不仅增强了结果的感性质量，而且也提高了OCR的性能。我们利用数据集Doc3D中的额外的真实信息（如表面发现，光照度图等）来训练这个优化网络。如图5所示，这个优化网络包含两个U-Net类型的编码解码结构：一个用于预测给定输入图像*I*的表面法向量N(H\*W\*3)，另一个网络将图像*I*和对应的法线向量N作为输入来预测一个阴影图S(H\*W\*3),S描述阴影的强度和颜色。那么，我们基于本质图像分解[2]来恢复任意阴影的图像A：I=A(\*)S，其中(\*)表示Hadamard product算子。更多的细节在补充材料中介绍。
  >
  > ![alt](D:\新建文件夹\OCR\论文\data\插图4.jpg)
  >
  > **4.2 损失函数**
  >
  > ​		训练过程分为两阶段，在第一阶段，形状网络和纹理映射网络分开进行初始化训练。在第二个阶段，两个子网络进行联合训练以提高矫正效果。为了方便，我们定义预测变量为X^，对应的真实值是X。形状网络优化的损失函数Lc如公式1所示：
  >
  > <img src="D:\新建文件夹\OCR\论文\data\公式1.png" alt="alt" style="zoom:50%;" />
  >
  > ​		其中，<img src="D:\新建文件夹\OCR\论文\data\公式1_1.png" alt="alt" style="zoom: 67%;" /> 是图像C的水平和竖直的梯度，lamda是控制梯度项的影响的因子。图像的梯度有助于学习到高频细节如图像的边缘(山脊和山谷)
  >
  > ​		纹理映射网络通过最小化公式2的LT进行训练。这个损失被定义为LB和LD的线性结合，LB代表反向映射B^的损失项，LD代表预测的矫正后的图像D^的损失项。公式2如下：
  >
  > <img src="D:\新建文件夹\OCR\论文\data\公式2.png" alt="alt" style="zoom:67%;" />
  >
  > ​		LD是矫正图像的重建损失。LB是像素坐标的回归损失。我们同时优化LB和LD。
  >
  > ​		在训练时，对于每个输入图像I，我们将对应的真实形变应用到规则的棋盘模式图像D上，获得一个对应的棋盘图像Ic. 我们使用预测的反向映射B^来矫正图像Ic，获得矫正后的棋盘图像D^来计算LD。棋盘纹理的目标是增强不同输入图像的LD的一致性，而不管文档本身的纹理(这句话的意思对应于3Dshape相同而文档内容纹理不同的图像矫正方式一样)。换句话讲，两张形变相同的图像应该使用相同的矫正方式而不管他们的文档内容，这意味着两张图像的LD相同。注意到Ic仅仅在训练时候才用到。
  >
  > ​		在第二个阶段，形状网络和纹理映射网络以端到端的方式同时进行训练。这种联合优化能够使反向映射损失弥补(补偿)形状网络的缺陷(???)。端到端训练的目标函数L是LD(3D坐标)和LT(纹理)加权线性组合。如下  
  >
  > <img src="D:\新建文件夹\OCR\论文\data\公式3.png" alt="alt" style="zoom: 33%;" />
  >
  > ​		对于优化网络的阴影去除任务，我们对S和S^使用L1损失<img src="D:\新建文件夹\OCR\论文\data\公式4.png" alt="alt" style="zoom:25%;" />
  >
  > 
  >
  > **4.3 训练细节**
  >
  > ​		我们在Doc3D上训练，将10万张图像分为训练集和验证集这样就不会有相同的网格图。在第一个阶段--初始化训练中，纹理映射网络使用真实的3D坐标图作为输入。在第二阶段--联合训练中，每个子网络都分别用最好分开训练的参数初始化，纹理映射网络的输入就是形状网络输出的预测的3D坐标图C^。B^在[-1,1]之间，C^在[0,1]之间。
  >
  > ​		我们使用多种数据增强技术：我们使用DTD[7]数据集和KTH2btips[6]数据集中的背景替换掉我们的训练数据的背景。训练数据的强度和颜色随机抖动(变化一点点)。
  >
  > ​		***超参数***     
  >
  > 

* **5、实验部分**

  > ​		我们在[23]的130张参考图像基准上进行了多次实验验证我们的方法，并在[45]的真实图像上给出了定性的结果。我们将DocUNet在我们的数据集上训练的结果作为基准线。此外，我们从文档分析的角度来评估我们的方法在OCR上的性能。最后，我们提供了一个详细的消融实验来显示Coordinate Convolutions、损失LD对矫正效果的影响。定性实验如图7所示。
  >
  > **5.1 实验设置**
  >
  > ​		***benchmark***  130张图像，6种不同复杂程度的形变类型。
  >
  > ​		***evaluation metrics***   使用两种类型的评价指标。（1）图像相似性指标MS-SSIM和LD（2）OCR性能
  >
  > **5.2 DocUNet在Doc3D上实验**
  >
  > 
  >
  > **5.3 在DocUNet的基准上测试DewarpNet**
  >
  > 
  >
  > **5.4 OCR评估**
  >
  > 
  >
  > **5.5 消融实验**
  >
  > 
  >
  > **5.6 定性评估**

* **6、展望**

  > ​		本文展示了一个新颖的用于文档图像矫正的深度学习框架DewarpNet。我们的方法对文档内容、光照、阴影和背景具有较高的鲁棒性。通过对3D形状进行建模，DewarpNet显示了超过最先进的方法优异性能。此外，我们制作了具有丰富标注的数据集Doc3D。
  >
  > ​		***方法中存在的一些问题***    首先，廉价的深度摄像头无法捕获变形的细节，如纸面的细微褶皱。 因此我们的数据集缺乏高度复杂的纸张褶皱样本。 未来，我们打算建立拥有更好细节和更复杂结构的数据集。  
  >
  > 第二个问题，DewarpNet是一个对遮挡相对敏感的算法：当文档图像部分被遮挡时网络结果会下降。未来，我们打算通过数据增强和对抗训练来解决这个问题。
  >
  > 